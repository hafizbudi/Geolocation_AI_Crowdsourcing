# -*- coding: utf-8 -*-
"""IMAGE_fine_tuningMV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VxVWmfDlTMX13mGy7QRHtGHjNkgTy3aj
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras import layers

!rm -rf ./*

print("Tensorflow version " + tf.__version__)
AUTO = tf.data.experimental.AUTOTUNE

#https://drive.google.com/drive/folders/1yc_2QZE4pZP8C97G2xOhd-uGf3WOhlOy?usp=sharing
from pydrive.auth import GoogleAuth
from google.colab import drive
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

!ls

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
#https://drive.google.com/file/d/1xrUMxvyZh4PQAukRdbFFm_ROIG3sq3Jp/view?usp=sharing
file_id = '1xrUMxvyZh4PQAukRdbFFm_ROIG3sq3Jp' #<-- You add in here the id from you google drive file, you can find it


download = drive.CreateFile({'id': file_id})


# Download the file to a local disc
download.GetContentFile('saved_model.pb.zip')

#df  = pd.read_csv("file.csv")
#df.head()

file_id = '1gZDhwnYfyMBXlQhF2k_UWb9KUiuNfRXy' #<-- You add in here the id from you google drive file, you can find it
#https://drive.google.com/file/d/1g467VvpaBLy6q8NpszcV7yCTAPNNHoGG/view?usp=sharing

#https://drive.google.com/file/d/1gZDhwnYfyMBXlQhF2k_UWb9KUiuNfRXy/view?usp=sharing

download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('easy.zip')

#https://drive.google.com/file/d/1EXyRAv3bDa38mlCucGEquyrt6RoKZXpn/view?usp=sharing
file_id='1EXyRAv3bDa38mlCucGEquyrt6RoKZXpn'
download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('medium.zip')


#https://drive.google.com/file/d/1dV40cnowdIFLxfAuUiBTmtIuLTKMC37l/view?usp=sharing
file_id='1dV40cnowdIFLxfAuUiBTmtIuLTKMC37l'
download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('difficult.zip')

#https://drive.google.com/file/d/1IgPC2x33rFRJBncvtwxemOzlNJfMlOmU/view?usp=sharing
file_id='1IgPC2x33rFRJBncvtwxemOzlNJfMlOmU'
download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('impossible.zip')

file_id='1g467VvpaBLy6q8NpszcV7yCTAPNNHoGG'
download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('variables.zip')

#https://drive.google.com/file/d/1ovweA8NDaUIyyjRfwI5SawMxm3DNdKEx/view?usp=sharing

file_id='1ovweA8NDaUIyyjRfwI5SawMxm3DNdKEx'
download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('keras_metadata.pb')

!unzip 'variables.zip'
!unzip 'easy.zip'
!unzip 'medium.zip'
!unzip 'difficult.zip'
!unzip 'impossible.zip'
!unzip 'saved_model.pb.zip'

os.chdir('/content/')
!rm -rf Yes
!rm -rf No
!rm -rf Data
!mkdir Yes
!mkdir No
!cp -f difficult/* No/
!cp -f impossible/* No/
!cp -f easy/* Yes/
!cp -f medium/* Yes/

!rm -rf Classes
!mkdir Classes
!mv Yes/ Classes/
!mv No/ Classes/

"""#Preliminary Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

import cv2
import os

labels = ['No', 'Yes']
img_size = 528
def get_data(data_dir):
    data = []
    for label in labels:
        path = os.path.join(data_dir, label)
        class_num = labels.index(label)
        for img in os.listdir(path):
            try:
                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format
                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size
                data.append([resized_arr, class_num])
            except Exception as e:
                print(e)
    return np.array(data)

train = get_data('./Classes/')

l = []
for i in train:
    if(i[1] == 0):
        l.append("No")
    else:
        l.append("Yes")
sns.set_style('darkgrid')
sns.countplot(l)

"""#balance classes"""

!pip install pillow

from PIL import Image, ImageEnhance
from PIL import ImageFilter
from glob import glob

for count in range(1):

  for imagefile in glob('/content/Classes/Yes/*'):
    #print(imagefile)
    im=Image.open(imagefile)
    im=im.convert("RGB")
    # Creating object of Brightness class
    b = ImageEnhance.Brightness(im)

    # showing resultant image
    b.enhance(1.5).save(imagefile+'_'+str(count)+'_b.jpeg')

    c = ImageEnhance.Contrast(im)
    c.enhance(0.8).save(imagefile+'_'+str(count)+'_c.jpeg')

    s = ImageEnhance.Color(im)
    s.enhance(2).save(imagefile+'_'+str(count)+'_s.jpeg')


    #r,g,b=im.split()
    #r=r.convert("RGB")
    #g=g.convert("RGB")
    #b=b.convert("RGB")
    #im_blur=im.filter(ImageFilter.GaussianBlur)
    im_unsharp=im.filter(ImageFilter.UnsharpMask)
    #im_blur.save(imagefile+'_'+str(count)+'_bl.jpeg')
    im_unsharp.save(imagefile+'_'+str(count)+'_un.jpeg')

!pip install split-folders
import splitfolders
splitfolders.ratio('/content/Classes/', output="output", seed=1234, ratio=(.7, .2,.1))
!mv output Data
PATH='/content/Data'
train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'val')
test_dir=os.path.join(PATH,'test')

train = get_data('./Data/train/')
val = get_data('./Data/val/')

l = []
for i in train:
    if(i[1] == 0):
        l.append("No")
    else:
        l.append("Yes")
sns.set_style('darkgrid')
sns.countplot(l)

!rm Data/test/Yes/*_0_*.jpeg #remove augmented data from the Test data set

!tar -zcvf Test.tgz Data/test/

"""#Transfer Learning EfficientNetB7 with noisy student weights"""

BATCH_SIZE = 16
IMG_SIZE = (529, 529)
#del train_dataset
#del validation_dataset
#del test_dataset
train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,
                                                            shuffle=True,
                                                            batch_size=BATCH_SIZE,
                                                            image_size=IMG_SIZE)
validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)
test_dataset = tf.keras.utils.image_dataset_from_directory(test_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)

class_names = train_dataset.class_names

plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

AUTOTUNE = tf.data.AUTOTUNE

train_dataset= train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

#del data_augmentation
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
])

for imag, label in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = imag[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')

preprocess_input = tf.keras.applications.efficientnet.preprocess_input

IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.efficientnet.EfficientNetB6(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')

image_batch, label_batch = next(iter(train_dataset))
feature_batch = base_model(image_batch)
print(feature_batch.shape)

base_model.trainable = False

# Let's take a look at the base model architecture
base_model.summary()



global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)

prediction_layer = tf.keras.layers.Dense(1)
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)

inputs = tf.keras.Input(shape=(529, 529, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

base_learning_rate = 0.001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

initial_epochs = 25

loss0, accuracy0 = model.evaluate(validation_dataset)

history = model.fit(train_dataset,
                    epochs=initial_epochs,
                    validation_data=validation_dataset)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

base_model.trainable = True

print("Number of layers in the base model: ", len(base_model.layers))

# Let's take a look to see how many layers are in the base model

# Fine-tune from this layer onwards
fine_tune_at = 600

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False

# Unfreeze the top X layers while leaving BatchNorm layers frozen results in:
for layer in base_model.layers[fine_tune_at:]:
    print(layer.name)
    if not isinstance(layer, tf.keras.layers.BatchNormalization):
        print("Not BatchNormalization layer:" + layer.name)
        layer.trainable = True


# model.summary()

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/100),
              metrics=['accuracy'])

len(model.trainable_variables)

fine_tune_epochs = 25
total_epochs =  initial_epochs + fine_tune_epochs
checkpoint_path = "./cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)


# This may generate warnings related to saving the state of the optimizer.
# These warnings (and similar warnings throughout this notebook)
# are in place to discourage outdated usage, and can be ignored.
history_fine = model.fit(train_dataset,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data=validation_dataset,
                         callbacks=[cp_callback])

acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']

loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

loss, accuracy = model.evaluate(test_dataset)
print('Test accuracy :', accuracy)

# Retrieve a batch of images from the test set
image_batch, label_batch = test_dataset.as_numpy_iterator().next()
predictions = model.predict_on_batch(image_batch).flatten()

# Apply a sigmoid since our model returns logits
predictions = tf.nn.sigmoid(predictions)
predictions = tf.where(predictions < 0.5, 0, 1)

print('Predictions:\n', predictions.numpy())
print('Labels:\n', label_batch)

plt.figure(figsize=(10, 10))
for i in range(9):
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(image_batch[i].astype("uint8"))
  plt.title(class_names[predictions[i]])
  plt.axis("off")

!pip install pyyaml h5py  # Required to save models in HDF5 format

!mkdir -p saved_model
model.save('saved_model/image_model_efnb6_imgnet')

!ls -l saved_model/image_model_efnb6_imgnet/

!tar -zcvf image_model_efnb6_imgnet.tgz saved_model/image_model_efnb6_imgnet/

from google.colab import files
files.download( "image_model_efnb6_imgnet.tgz" )

"""#Improvements TBC

#Use noisy-student weights
"""

!wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/noisystudent/noisy_student_efficientnet-b6.tar.gz

!tar -xf noisy_student_efficientnet-b6.tar.gz

!wget https://raw.githubusercontent.com/keras-team/keras/master/keras/applications/efficientnet_weight_update_util.py



!python efficientnet_weight_update_util.py --model b6 --notop --ckpt noisy_student_efficientnet-b6/model.ckpt --o efficientnetb6_notop.h5



IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.efficientnet.EfficientNetB6(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='efficientnetb6_notop.h5')

image_batch, label_batch = next(iter(train_dataset))
feature_batch = base_model(image_batch)
print(feature_batch.shape)

base_model.trainable = False

# Let's take a look at the base model architecture
base_model.summary()



global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)

prediction_layer = tf.keras.layers.Dense(1)
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)

inputs = tf.keras.Input(shape=(529, 529, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

base_learning_rate = 0.001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

initial_epochs = 25

loss0, accuracy0 = model.evaluate(validation_dataset)

history = model.fit(train_dataset,
                    epochs=initial_epochs,
                    validation_data=validation_dataset)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

base_model.trainable = True

print("Number of layers in the base model: ", len(base_model.layers))

# Let's take a look to see how many layers are in the base model

# Fine-tune from this layer onwards
fine_tune_at = 600

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False

# Unfreeze the top X layers while leaving BatchNorm layers frozen results in:
for layer in base_model.layers[fine_tune_at:]:
    print(layer.name)
    if not isinstance(layer, tf.keras.layers.BatchNormalization):
        print("Not BatchNormalization layer:" + layer.name)
        layer.trainable = True


# model.summary()

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/100),
              metrics=['accuracy'])

len(model.trainable_variables)

fine_tune_epochs = 25
total_epochs =  initial_epochs + fine_tune_epochs
checkpoint_path = "./cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)


# This may generate warnings related to saving the state of the optimizer.
# These warnings (and similar warnings throughout this notebook)
# are in place to discourage outdated usage, and can be ignored.
history_fine = model.fit(train_dataset,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data=validation_dataset,
                         callbacks=[cp_callback])

acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']

loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

loss, accuracy = model.evaluate(test_dataset)
print('Test accuracy :', accuracy)

# Retrieve a batch of images from the test set
image_batch, label_batch = test_dataset.as_numpy_iterator().next()
predictions = model.predict_on_batch(image_batch).flatten()

# Apply a sigmoid since our model returns logits
predictions = tf.nn.sigmoid(predictions)
predictions = tf.where(predictions < 0.5, 0, 1)

print('Predictions:\n', predictions.numpy())
print('Labels:\n', label_batch)

plt.figure(figsize=(10, 10))
for i in range(9):
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(image_batch[i].astype("uint8"))
  plt.title(class_names[predictions[i]])
  plt.axis("off")

!pip install pyyaml h5py  # Required to save models in HDF5 format

!mkdir -p saved_model
model.save('saved_model/image_model_efnb6_noisy')

!ls -l saved_model/image_model_efnb6_noisy/

!tar -zcvf image_model_efnb6_noisy.tgz saved_model/image_model_efnb6_noisy/

from google.colab import files
files.download( "image_model_efnb6_noisy.tgz" )

from google.colab import files
files.download( "Test.tgz" )

"""#LOAD MODEL and PREDICT

"""



#https://drive.google.com/drive/folders/1yc_2QZE4pZP8C97G2xOhd-uGf3WOhlOy?usp=sharing
from pydrive.auth import GoogleAuth
from google.colab import drive
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

!rm -rf *
!ls

#https://drive.google.com/file/d/1eOgIU0RzqMTd0ZsKQaRZOxvt7ktK29WY/view?usp=sharing
#https://drive.google.com/file/d/1U-Xmy9IPyeRzhrd9E-fd-UHBy8OF7KMg/view?usp=sharing
#https://drive.google.com/file/d/1eOgIU0RzqMTd0ZsKQaRZOxvt7ktK29WY/view?usp=sharing
#https://drive.google.com/file/d/1eOgIU0RzqMTd0ZsKQaRZOxvt7ktK29WY/view?usp=sharing
#https://drive.google.com/file/d/11fcgOCvheAK95fh01z6EPXubZjLjHcov/view?usp=sharing
#https://drive.google.com/file/d/11fcgOCvheAK95fh01z6EPXubZjLjHcov/view?usp=sharing
file_id='11fcgOCvheAK95fh01z6EPXubZjLjHcov' #DS
download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('image_model_efnb6_noisy.tgz')

!tar -zxvf 'image_model_efnb6_noisy.tgz'

file_id = '1gZDhwnYfyMBXlQhF2k_UWb9KUiuNfRXy' #<-- You add in here the id from you google drive file, you can find it
#https://drive.google.com/file/d/1g467VvpaBLy6q8NpszcV7yCTAPNNHoGG/view?usp=sharing

#https://drive.google.com/file/d/1gZDhwnYfyMBXlQhF2k_UWb9KUiuNfRXy/view?usp=sharing

download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('easy.zip')

#https://drive.google.com/file/d/1EXyRAv3bDa38mlCucGEquyrt6RoKZXpn/view?usp=sharing
file_id='1EXyRAv3bDa38mlCucGEquyrt6RoKZXpn'
download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('medium.zip')


#https://drive.google.com/file/d/1dV40cnowdIFLxfAuUiBTmtIuLTKMC37l/view?usp=sharing
file_id='1dV40cnowdIFLxfAuUiBTmtIuLTKMC37l'
download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('difficult.zip')

#https://drive.google.com/file/d/1IgPC2x33rFRJBncvtwxemOzlNJfMlOmU/view?usp=sharing
file_id='1IgPC2x33rFRJBncvtwxemOzlNJfMlOmU'
download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('impossible.zip')

file_id='1g467VvpaBLy6q8NpszcV7yCTAPNNHoGG'
download = drive.CreateFile({'id': file_id})

!unzip 'easy.zip'
!unzip 'medium.zip'
!unzip 'difficult.zip'
!unzip 'impossible.zip'

os.chdir('/content/')
!rm -rf Yes
!rm -rf No
!rm -rf Data
!mkdir Yes
!mkdir No
!cp -f difficult/* No/
!cp -f impossible/* No/
!cp -f easy/* Yes/
!cp -f medium/* Yes/

!rm -rf Classes
!mkdir Classes
!mv Yes/ Classes/
!mv No/ Classes/

from PIL import Image, ImageEnhance
from PIL import ImageFilter
from glob import glob

for count in range(1):

  for imagefile in glob('/content/Classes/Yes/*'):
    #print(imagefile)
    im=Image.open(imagefile)
    im=im.convert("RGB")
    # Creating object of Brightness class
    b = ImageEnhance.Brightness(im)

    # showing resultant image
    b.enhance(1.5).save(imagefile+'_'+str(count)+'_b.jpeg')

    c = ImageEnhance.Contrast(im)
    c.enhance(0.8).save(imagefile+'_'+str(count)+'_c.jpeg')

    s = ImageEnhance.Color(im)
    s.enhance(2).save(imagefile+'_'+str(count)+'_s.jpeg')


    #r,g,b=im.split()
    #r=r.convert("RGB")
    #g=g.convert("RGB")
    #b=b.convert("RGB")
    #im_blur=im.filter(ImageFilter.GaussianBlur)
    im_unsharp=im.filter(ImageFilter.UnsharpMask)
    #im_blur.save(imagefile+'_'+str(count)+'_bl.jpeg')
    im_unsharp.save(imagefile+'_'+str(count)+'_un.jpeg')

!pip install split-folders
import splitfolders
splitfolders.ratio('/content/Classes/', output="output", seed=1234, ratio=(.7, .2,.1))
!mv output Data
PATH='/content/Data'
train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'val')
test_dir=os.path.join(PATH,'test')

!rm Data/test/Yes/*_0_*.jpeg #remove augmented data from the Test data set

BATCH_SIZE = 16
IMG_SIZE = (529, 529)
#del train_dataset
#del validation_dataset
#del test_dataset
train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,
                                                            shuffle=True,
                                                            batch_size=BATCH_SIZE,
                                                            image_size=IMG_SIZE)
validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)
test_dataset = tf.keras.utils.image_dataset_from_directory(test_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)

class_names = test_dataset.class_names

files=test_dataset.file_paths
print(files)
for images, labels in test_dataset.take(6):

  plt.figure(figsize=(10, 10))
  for i in range(16):

    ax = plt.subplot(4, 4, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")



from tensorflow.keras.models import load_model
model = load_model('./saved_model/image_model_efnb6_noisy/')

class_names = test_dataset.class_names
# Retrieve a batch of images from the test set
#image_batch, label_batch = test_dataset.as_numpy_iterator().next()
#predictions = model.predict_on_batch(image_batch).flatten()

#predictions=model.predict(test_dataset).flatten()
#predictions = tf.nn.sigmoid(predictions)
#predictions=tf.where(predictions<0.5,0,1)
o=0
count=0
for images, labels in test_dataset.take(6):
  plt.figure(figsize=(10, 10))
  for i in range(16):
    o+=1
    ax = plt.subplot(4, 4, i + 1)
    predictions=model.predict(np.expand_dims(images[i],0)).flatten()
    predictions=tf.nn.sigmoid(predictions)
    #print(predictions[0])
    #predictions = tf.nn.sigmoid(predictions)
    if predictions[0]<=0.5:
      prediction=0
    else:
      prediction=1
    print('pred: '+str(prediction))
    plt.imshow(images[i].numpy().astype("uint8"))
    if class_names[labels[i]]!=class_names[prediction]:
      plt.title(class_names[labels[i]]+', '+class_names[prediction],color='red')
    else:
      plt.title(class_names[labels[i]]+', '+class_names[prediction],color='blue')
    plt.axis("off")

class_names = test_dataset.class_names
# Retrieve a batch of images from the test set
#image_batch, label_batch = test_dataset.as_numpy_iterator().next()
#predictions = model.predict_on_batch(image_batch).flatten()

predictions=model.predict(test_dataset).flatten()
predictions = tf.nn.sigmoid(predictions)
predictions=tf.where(predictions<0.5,0,1)
o=0
count=0
for images, labels in test_dataset.take(6):
  plt.figure(figsize=(10, 10))
  for i in range(16):
    o+=1
    ax = plt.subplot(4, 4, i + 1)
    predictions=model.predict(np.expand_dims(images[i],0)).flatten()
    print(predictions)
    #predictions = tf.nn.sigmoid(predictions)
    if predictions[0]<=0.5:
      prediction=0
    else:
      prediction=1
    plt.imshow(images[i].numpy().astype("uint8"))
    if class_names[labels[i]]!=class_names[prediction]:
      plt.title(class_names[labels[i]]+', '+class_names[prediction],color='red')
    else:
      plt.title(class_names[labels[i]]+', '+class_names[prediction],color='blue')
    plt.axis("off")

